### 隐马尔科夫模型基本概念
1. 隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐马尔科夫模型由初始状态概率向量、状态转移概率矩阵和观测概率矩阵（也可称为发射概率矩阵）组成
2. 隐马尔科夫模型做了两个基本假设——一是任意时刻的隐状态只与前一时刻的隐状态有关，二是任意时刻的观测只与当前时刻的隐状态有关
3. 隐马尔科夫模型有3个基本问题：
（1）概率计算问题。给定模型参数和观测序列，计算在模型下观测序列出现的概率  
（2）学习问题。也就是模型的训练，已知观测序列，估计模型参数  
（3）预测问题。也称为decode，给定观测序列，求最有可能的对应的隐状态
4. 使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列
5. 对于HMM模型，首先我们假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：  
$Q = \{q_1,q_2,...,q_N\}, \; V =\{v_1,v_2,...v_M\}$
其中，N是可能的隐藏状态数，M是所有的可能的观察状态数。对于一个长度为T的序列，I对应的状态序列, O是对应的观察序列，即：  
$I = \{i_1,i_2,...,i_T\}, \; O =\{o_1,o_2,...o_T\}$  
其中，任意一个隐藏状态$i_t \in Q$,任意一个观察状态$o_t \in V$
6. HMM模型做了两个很重要的假设如下：  
(1) 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻t的隐藏状态是$i_t= q_i$,在时刻$t+1$的隐藏状态是$i_{t+1} = q_j$,则从时刻$t$到时刻$t+1$的HMM状态转移概率$a_{ij}$可以表示为：  
$a_{ij} = P(i_{t+1} = q_j | i_t= q_i)$  
这样$a_{i,j}$可以组成马尔科夫链的状态转移矩阵A：  
$A=\Big [a_{ij}\Big ]_{N \times N}$  
(2)观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻t的隐藏状态是$i_t= q_j$, 而对应的观察状态为$o_t=v_k$, 则该时刻观察状态$v_k$在隐藏状态$q_j$下生成的概率为$b_j(k)$,满足：  
$b_j(k) = P(o_t = v_k | i_t= q_j)$  
这样$b_j(k)$可以组成观测状态生成的概率矩阵B:  
$B = \Big [b_j(k) \Big ]_{N \times M}$  
除此之外，我们需要一组在时刻$t=1$的隐藏状态概率分布$\Pi$:  
$\Pi = \Big [ \pi(i)\Big ]_N \; 其中 \;\pi(i) = P(i_1 = q_i)$  
一个HMM模型，可以由隐藏状态初始概率分布$\Pi$, 状态转移概率矩阵$A$和观测状态概率矩阵$B$决定。$\Pi$,$A$决定状态序列，$B$决定观测序列。因此，HMM模型可以由一个三元组$\lambda$表示如下：  
$\lambda=(A,B,\Pi)$



