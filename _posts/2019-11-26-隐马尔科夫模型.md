### 隐马尔科夫模型基本概念
1. 隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐马尔科夫模型由初始状态概率向量、状态转移概率矩阵和观测概率矩阵（也可称为发射概率矩阵）组成
2. 隐马尔科夫模型做了两个基本假设——一是任意时刻的隐状态只与前一时刻的隐状态有关，二是任意时刻的观测只与当前时刻的隐状态有关
3. 隐马尔科夫模型有3个基本问题：
（1）概率计算问题。给定模型参数和观测序列，计算在模型下观测序列出现的概率  
（2）学习问题。也就是模型的训练，已知观测序列，估计模型参数  
（3）预测问题。也称为decode，给定观测序列，求最有可能的对应的隐状态
4. 使用HMM模型时我们的问题一般有这两个特征：１）我们的问题是基于序列的，比如时间序列，或者状态序列。２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列
5. 对于HMM模型，首先我们假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：  
$Q = \{q_1,q_2,...,q_N\}, \; V =\{v_1,v_2,...v_M\}$
其中，N是可能的隐藏状态数，M是所有的可能的观察状态数。对于一个长度为T的序列，I对应的状态序列, O是对应的观察序列，即：  
$I = \{i_1,i_2,...,i_T\}, \; O =\{o_1,o_2,...o_T\}$  
其中，任意一个隐藏状态$i_t \in Q$,任意一个观察状态$o_t \in V$
6. HMM模型做了两个很重要的假设如下：  
(1) 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻t的隐藏状态是$i_t= q_i$,在时刻$t+1$的隐藏状态是$i_{t+1} = q_j$,则从时刻$t$到时刻$t+1$的HMM状态转移概率$a_{ij}$可以表示为：  
$a_{ij} = P(i_{t+1} = q_j | i_t= q_i)$  
这样$a_{i,j}$可以组成马尔科夫链的状态转移矩阵A：  
$A=\Big [a_{ij}\Big ]_{N \times N}$  
(2)观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻t的隐藏状态是$i_t= q_j$, 而对应的观察状态为$o_t=v_k$, 则该时刻观察状态$v_k$在隐藏状态$q_j$下生成的概率为$b_j(k)$,满足：  
$b_j(k) = P(o_t = v_k | i_t= q_j)$  
这样$b_j(k)$可以组成观测状态生成的概率矩阵B:  
$B = \Big [b_j(k) \Big ]_{N \times M}$  
除此之外，我们需要一组在时刻$t=1$的隐藏状态概率分布$\Pi$:  
$\Pi = \Big [ \pi(i)\Big ]_N \; 其中 \;\pi(i) = P(i_1 = q_i)$  
一个HMM模型，可以由隐藏状态初始概率分布$\Pi$, 状态转移概率矩阵$A$和观测状态概率矩阵$B$决定。$\Pi$,$A$决定状态序列，$B$决定观测序列。因此，HMM模型可以由一个三元组$\lambda$表示如下：  
$\lambda=(A,B,\Pi)$
7. HMM模型一共有三个经典的问题需要解决：  
(1)评估观察序列概率。即给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。这个问题的求解需要用到前向后向算法，这个问题是HMM模型三个问题中最简单的  
(2)模型参数学习问题。即给定观测序列$O =\{o_1,o_2,...o_T\}$,估计模型$\lambda = (A, B, \Pi)$的参数，使该模型下观测序列的条件概率$P(O|\lambda)$最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法，这个问题是HMM模型三个问题中最复杂的  
(3)预测问题，也称为解码问题。即给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法。这个问题是HMM模型三个问题中复杂度居中的算法



